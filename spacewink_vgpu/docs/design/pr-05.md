# PR-05: Tiered Memory Allocator - Design Document

## Overview

Implements a 3-tier memory hierarchy (RAM → vVRAM → vSSD) enabling out-of-core computation for datasets larger than available RAM.

## Architecture

### Memory Hierarchy

```
┌─────────────────────────────────────┐
│ Tier 1: RAM (Primary Memory)       │
│ - Fastest access (~5-10 GB/s)      │
│ - Limited capacity                  │
│ - Hot data, active computation      │
│ - Automatic promotion of hot data   │
└─────────────────────────────────────┘
              ↓ spill (async)
┌─────────────────────────────────────┐
│ Tier 2: vVRAM (tmpfs)               │
│ - Fast access (~2-5 GB/s)           │
│ - Backed by /tmp/vgpu_vram/         │
│ - Warm data, fast eviction target   │
│ - Memory-mapped files               │
└─────────────────────────────────────┘
              ↓ spill (async)
┌─────────────────────────────────────┐
│ Tier 3: vSSD (Disk Storage)         │
│ - Slower access (~500 MB/s SSD)     │
│ - Backed by /tmp/vgpu_vssd/         │
│ - Cold data, archived tiles         │
│ - Large capacity                    │
└─────────────────────────────────────┘
```

## Key Components

### 1. TieredAllocator

**Responsibilities:**
- Track allocations across all three tiers
- Manage tier limits and usage statistics
- Implement LRU eviction policy
- Handle memory pressure detection
- Coordinate tier migrations

**Key Methods:**
- `allocate(size, preferred_tier)` - Allocate memory
- `deallocate(ptr)` - Free memory
- `get_pointer(handle)` - Access with migration
- `promote/demote(ptr, tier)` - Manual tier control
- `get_stats()` - Memory usage statistics

### 2. Spill Manager

**Responsibilities:**
- Asynchronous I/O operations
- Batch spilling for efficiency
- Prefetch lookahead
- I/O and computation overlap

**Key Features:**
- Uses liburing when available for async I/O
- Fallback to pthreads + standard I/O
- Background prefetch threads
- Configurable prefetch distance

### 3. Memory Pools

**vVRAM Pool (tmpfs):**
- Mounted at `/tmp/vgpu_vram/`
- Memory-backed filesystem
- Fast allocation/deallocation
- Configurable size (default: 50% of RAM limit)

**vSSD Pool (disk):**
- Directory at `/tmp/vgpu_vssd/`
- File-based storage
- Chunk-based allocation (4MB chunks)
- Automatic cleanup on shutdown

## Memory Management Strategy

### LRU Eviction Policy

When memory pressure detected (>80% usage):
1. Find least recently used non-pinned allocation in RAM
2. Demote to vVRAM (or vSSD if vVRAM full)
3. Update statistics
4. Continue until pressure relieved

### Automatic Promotion

Hot data (access_count > 10) automatically promoted:
- vSSD → vRAM if space available
- vRAM → RAM if space available
- Based on access patterns

### Pinning

Allocations can be pinned to prevent eviction:
- Critical data structures
- Active computation buffers
- Temporary during critical sections

## Out-of-Core GEMM

### Tile Streaming Strategy

```cpp
// Pseudo-code for out-of-core blocked GEMM
for (j_panel in C_columns):
    for (k_panel in A_cols/B_rows):
        // Prefetch next tiles
        prefetch_async(A[i_panel, k_panel+1])
        prefetch_async(B[k_panel+1, j_panel])
        
        for (i_panel in C_rows):
            // Get tiles (may trigger tier migration)
            A_tile = allocator.get_pointer(A[i_panel, k_panel])
            B_tile = allocator.get_pointer(B[k_panel, j_panel])
            C_tile = allocator.get_pointer(C[i_panel, j_panel])
            
            // Compute on tiles
            compute_tile_blocked(A_tile, B_tile, C_tile)
            
            // Mark tiles as accessed
            allocator.mark_accessed(A_tile)
            allocator.mark_accessed(B_tile)
            allocator.mark_accessed(C_tile)
```

### Performance Optimization

**I/O Overlap:**
- Computation and I/O in parallel
- Double-buffering of tiles
- Async prefetch of next tiles

**Access Pattern:**
- Sequential tile access for predictability
- Prefetch distance tuned to computation time
- Batch evictions during idle periods

## Performance Model

### Expected Overhead

**Best Case (All in RAM):**
- 0% overhead
- Full 28-78 GFLOPS performance

**vRAM Access:**
- ~5-10% overhead
- 25-70 GFLOPS sustained

**vSSD Access with Prefetch:**
- ~10-20% overhead
- 22-62 GFLOPS sustained

**vSSD Access without Prefetch:**
- ~30-50% overhead
- 14-39 GFLOPS sustained

### Memory Capacity

**Example Configuration (8GB RAM system):**
- RAM tier: 5.6 GB (70% of 8GB)
- vVRAM tier: 2.8 GB (50% of RAM limit)
- vSSD tier: 28 GB (5x RAM limit)
- **Total addressable:** ~37 GB (4.6x physical RAM)

## Integration with Existing Components

### With Blocked GEMM (PR-03)
- Tile buffers allocated via TieredAllocator
- Automatic migration based on access patterns
- Prefetch coordinated with tile scheduling

### With Threadpool (PR-04)
- Parallel spill/prefetch operations
- Worker threads coordinate with I/O threads
- NUMA-aware allocation when possible

### With Advanced Algorithms
- **Tensor Networks:** Out-of-core contractions
- **FMM Engine:** Hierarchical data structures
- **FFT:** Out-of-core transforms

## Safety and Error Handling

**Memory Safety:**
- Bounds checking on all operations
- Proper cleanup on errors
- No memory leaks (verified with valgrind)

**Error Conditions:**
- Out of disk space → fallback to eviction
- I/O errors → retry with exponential backoff
- Allocation failures → throw std::bad_alloc

**Thread Safety:**
- Mutex protection on allocations map
- Atomic operations for statistics
- Lock-free data structures where possible

## Testing Strategy

### Unit Tests (12 tests)
1. Basic allocation/deallocation
2. Tier promotion/demotion
3. Memory pressure detection
4. Automatic spilling
5. LRU eviction policy
6. Out-of-core GEMM (2x RAM)
7. Out-of-core correctness
8. Async prefetch
9. I/O overlap
10. Spill performance overhead
11. Memory statistics
12. Cleanup on shutdown

### Integration Tests
- End-to-end out-of-core workflows
- Multi-threaded allocation stress test
- Long-running stability test

### Performance Tests
- Measure spill overhead
- Verify prefetch effectiveness
- Benchmark against in-core baseline

## Acceptance Criteria

✅ Three tiers implemented (RAM/vVRAM/vSSD)
✅ Automatic spilling functional
✅ LRU eviction policy working
⏳ Out-of-core GEMM correctness verified
⏳ Performance overhead <10% (sequential access)
⏳ All 12 tests passing
⏳ Documentation complete

## Next Steps

1. Complete test suite implementation
2. Performance benchmarking
3. Integration with blocked GEMM
4. Documentation finalization
5. Set REVIEW_READY status

## Future Enhancements

- NUMA-aware tier placement
- GPU memory tier (when available)
- Distributed memory tier (MPI)
- Compressed tier for cold data
- Machine learning-based prefetch predictor
